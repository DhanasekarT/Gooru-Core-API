#-------------------------------------------------------------------------------
#  crawler4j.properties
#  gooru-api
#  Created by Gooru on 2014
#  Copyright (c) 2014 Gooru. All rights reserved.
#  http://www.goorulearning.org/
#  Permission is hereby granted, free of charge, to any person obtaining
#  a copy of this software and associated documentation files (the
#  "Software"), to deal in the Software without restriction, including
#  without limitation the rights to use, copy, modify, merge, publish,
#  distribute, sublicense, and/or sell copies of the Software, and to
#  permit persons to whom the Software is furnished to do so, subject to
#  the following conditions:
#  The above copyright notice and this permission notice shall be
#  included in all copies or substantial portions of the Software.
#  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
#  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
#  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
#  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
#  LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
#  OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
#  WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#-------------------------------------------------------------------------------
# Maximum Connections per host
fetcher.max_connections_per_host=1

# Maximum total connections
fetcher.max_total_connections=100

# Socket timeout in milliseconds
fetcher.socket_timeout=20000

# Connection timeout in milliseconds
fetcher.connection_timeout=30000

# Max number of outgoing links which are processed from a page
fetcher.max_outlinks=5000

# Max size of page is 1Mb
fetcher.max_download_size=100048576

# user-agent
fetcher.user_agent=crawler4j (http://code.google.com/p/crawler4j/)
fetcher.user_agent_name=crawler4j

# Default politeness delay in milliseconds
fetcher.default_politeness_delay=200

# Should we also crawl https pages?
fetcher.crawl_https=false

# Should we follow redirects?
fetcher.follow_redirects=true

# Should we log the 404 (Not Found) pages?
logging.show_404_pages=true

# Should we crawl images?
crawler.include_images=true

# Default encoding of pages
crawler.default_encoding=UTF-8

# Maximum depth of crawling
# For unlimited depth this parameter should be set to -1
crawler.max_depth=-1

# Maximum number of pages to fetch
# For unlimited number of pages, this parameter should be set to -1
crawler.max_pages_to_fetch=-1

# Should the crawler obey Robots.txt protocol?
# More info on Robots.txt is available at http://www.robotstxt.org/
crawler.obey_robotstxt=false
